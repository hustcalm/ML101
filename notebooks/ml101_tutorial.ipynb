{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4833e30",
   "metadata": {},
   "source": [
    "# ML101: Classical Machine Learning Algorithms\n",
    "\n",
    "This notebook provides an interactive exploration of classical machine learning algorithms implemented from scratch.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Linear Regression](#linear-regression)\n",
    "2. [Logistic Regression](#logistic-regression)\n",
    "3. [K-Nearest Neighbors](#k-nearest-neighbors)\n",
    "4. [Data Preprocessing](#data-preprocessing)\n",
    "5. [Model Evaluation](#model-evaluation)\n",
    "6. [Algorithm Comparison](#algorithm-comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be7e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import our ML101 implementations\n",
    "from ml101 import (\n",
    "    LinearRegression, LogisticRegression, KNearestNeighbors,\n",
    "    StandardScaler, MinMaxScaler, train_test_split as ml101_train_test_split\n",
    ")\n",
    "from ml101.utils.metrics import ClassificationMetrics, RegressionMetrics\n",
    "from ml101.utils.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries loaded successfully!\")\n",
    "print(f\"üì¶ ML101 version: {getattr(__import__('ml101'), '__version__', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8cfd06",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Linear regression is a fundamental algorithm for predicting continuous target variables. Let's explore both the Normal Equation and Gradient Descent methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbdcbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic linear data\n",
    "X, y = make_regression(n_samples=200, n_features=1, noise=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(f\"Feature dimensions: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6ba829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Normal Equation vs Gradient Descent\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Method 1: Normal Equation\n",
    "model_normal = LinearRegression(method='normal')\n",
    "model_normal.fit(X_train, y_train)\n",
    "\n",
    "# Method 2: Gradient Descent\n",
    "model_gd = LinearRegression(method='gradient_descent', learning_rate=0.01, max_iterations=1000)\n",
    "model_gd.fit(X_train, y_train)\n",
    "\n",
    "# Create prediction line\n",
    "X_plot = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "y_plot_normal = model_normal.predict(X_plot)\n",
    "y_plot_gd = model_gd.predict(X_plot)\n",
    "\n",
    "# Plot 1: Normal Equation\n",
    "axes[0].scatter(X_train, y_train, alpha=0.6, label='Training Data')\n",
    "axes[0].scatter(X_test, y_test, alpha=0.6, label='Test Data', color='red')\n",
    "axes[0].plot(X_plot, y_plot_normal, color='green', linewidth=2, label='Normal Equation')\n",
    "axes[0].set_title('Linear Regression - Normal Equation')\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Gradient Descent\n",
    "axes[1].scatter(X_train, y_train, alpha=0.6, label='Training Data')\n",
    "axes[1].scatter(X_test, y_test, alpha=0.6, label='Test Data', color='red')\n",
    "axes[1].plot(X_plot, y_plot_gd, color='orange', linewidth=2, label='Gradient Descent')\n",
    "axes[1].set_title('Linear Regression - Gradient Descent')\n",
    "axes[1].set_xlabel('X')\n",
    "axes[1].set_ylabel('y')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Cost History\n",
    "if model_gd.cost_history:\n",
    "    axes[2].plot(model_gd.cost_history, color='purple')\n",
    "    axes[2].set_title('Cost Function During Training')\n",
    "    axes[2].set_xlabel('Iteration')\n",
    "    axes[2].set_ylabel('Mean Squared Error')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print results\n",
    "print(\"\\nResults Comparison:\")\n",
    "print(f\"Normal Equation    - Weight: {model_normal.weights[0]:.4f}, Bias: {model_normal.bias:.4f}\")\n",
    "print(f\"Gradient Descent   - Weight: {model_gd.weights[0]:.4f}, Bias: {model_gd.bias:.4f}\")\n",
    "print(f\"\\nR¬≤ Scores:\")\n",
    "print(f\"Normal Equation    - Train: {model_normal.score(X_train, y_train):.4f}, Test: {model_normal.score(X_test, y_test):.4f}\")\n",
    "print(f\"Gradient Descent   - Train: {model_gd.score(X_train, y_train):.4f}, Test: {model_gd.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4c9187",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic regression is used for binary and multiclass classification problems. It uses the sigmoid function to model probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8185551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification data\n",
    "X_clf, y_clf = make_classification(n_samples=300, n_features=2, n_redundant=0, \n",
    "                                  n_informative=2, n_clusters_per_class=1, \n",
    "                                  random_state=42)\n",
    "X_clf_train, X_clf_test, y_clf_train, y_clf_test = train_test_split(X_clf, y_clf, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features for better convergence\n",
    "scaler = StandardScaler()\n",
    "X_clf_train_scaled = scaler.fit_transform(X_clf_train)\n",
    "X_clf_test_scaled = scaler.transform(X_clf_test)\n",
    "\n",
    "print(f\"Classification dataset shape: {X_clf.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_clf))}\")\n",
    "print(f\"Class distribution: {np.bincount(y_clf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f0898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression\n",
    "log_model = LogisticRegression(learning_rate=0.1, max_iterations=1000)\n",
    "log_model.fit(X_clf_train_scaled, y_clf_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = log_model.predict(X_clf_test_scaled)\n",
    "y_proba = log_model.predict_proba(X_clf_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = ClassificationMetrics.accuracy_score(y_clf_test, y_pred)\n",
    "precision, recall, fscore = ClassificationMetrics.precision_recall_fscore(y_clf_test, y_pred, average='binary')\n",
    "\n",
    "print(f\"\\nLogistic Regression Results:\")\n",
    "print(f\"Training Accuracy: {log_model.score(X_clf_train_scaled, y_clf_train):.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {fscore:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a6aa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize logistic regression results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot 1: Decision boundary\n",
    "def plot_decision_boundary(model, X, y, scaler, ax, title):\n",
    "    h = 0.1\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = model.predict_proba(mesh_points)[:, 1]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    contour = ax.contourf(xx, yy, Z, levels=50, alpha=0.6, cmap=plt.cm.RdYlBu)\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolors='black')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Feature 1 (scaled)')\n",
    "    ax.set_ylabel('Feature 2 (scaled)')\n",
    "    return contour\n",
    "\n",
    "contour = plot_decision_boundary(log_model, X_clf_train_scaled, y_clf_train, scaler, axes[0], \n",
    "                                'Logistic Regression Decision Boundary')\n",
    "plt.colorbar(contour, ax=axes[0])\n",
    "\n",
    "# Plot 2: Cost history\n",
    "if log_model.cost_history:\n",
    "    axes[1].plot(log_model.cost_history, color='blue')\n",
    "    axes[1].set_title('Cost Function History')\n",
    "    axes[1].set_xlabel('Iteration')\n",
    "    axes[1].set_ylabel('Logistic Loss')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Confusion Matrix\n",
    "cm = ClassificationMetrics.confusion_matrix(y_clf_test, y_pred)\n",
    "im = axes[2].imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "axes[2].figure.colorbar(im, ax=axes[2])\n",
    "axes[2].set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'],\n",
    "           title='Confusion Matrix', ylabel='True label', xlabel='Predicted label')\n",
    "\n",
    "# Add text annotations to confusion matrix\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        axes[2].text(j, i, format(cm[i, j], 'd'), ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a85de2a",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors\n",
    "\n",
    "KNN is a simple, instance-based learning algorithm that classifies instances based on the majority class of their k nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8e2bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate KNN data\n",
    "X_knn, y_knn = make_classification(n_samples=300, n_features=2, n_redundant=0, \n",
    "                                  n_informative=2, n_clusters_per_class=1, \n",
    "                                  random_state=42)\n",
    "X_knn_train, X_knn_test, y_knn_train, y_knn_test = train_test_split(X_knn, y_knn, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"KNN dataset shape: {X_knn.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_knn))}\")\n",
    "print(f\"Class distribution: {np.bincount(y_knn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e96fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze effect of different k values\n",
    "k_values = range(1, 21)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn_model = KNearestNeighbors(k=k)\n",
    "    knn_model.fit(X_knn_train, y_knn_train)\n",
    "    \n",
    "    train_score = knn_model.score(X_knn_train, y_knn_train)\n",
    "    test_score = knn_model.score(X_knn_test, y_knn_test)\n",
    "    \n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "\n",
    "# Plot k-value analysis\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, train_scores, 'o-', label='Training Accuracy', color='blue')\n",
    "plt.plot(k_values, test_scores, 'o-', label='Test Accuracy', color='red')\n",
    "plt.xlabel('k (Number of Neighbors)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('KNN Performance vs k Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Find best k\n",
    "best_k = k_values[np.argmax(test_scores)]\n",
    "print(f\"Best k value: {best_k} (Test Accuracy: {max(test_scores):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32e26cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize KNN decision boundaries for different k values\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "k_demo_values = [1, 3, 5, 10, 15, 25]\n",
    "\n",
    "def plot_knn_boundary(model, X, y, ax, title):\n",
    "    h = 0.1\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = model.predict(mesh_points)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, alpha=0.6, cmap=plt.cm.RdYlBu)\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolors='black')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "\n",
    "for i, k in enumerate(k_demo_values):\n",
    "    knn_model = KNearestNeighbors(k=k)\n",
    "    knn_model.fit(X_knn_train, y_knn_train)\n",
    "    \n",
    "    test_acc = knn_model.score(X_knn_test, y_knn_test)\n",
    "    plot_knn_boundary(knn_model, X_knn_train, y_knn_train, axes[i], \n",
    "                     f'KNN (k={k})\\nAccuracy: {test_acc:.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885f218d",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Data preprocessing is crucial for machine learning. Let's explore different scaling methods and feature engineering techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec77b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with different scales\n",
    "np.random.seed(42)\n",
    "X_preprocess = np.random.randn(200, 3) * [100, 10, 1] + [500, 50, 5]\n",
    "\n",
    "# Apply different scaling methods\n",
    "standard_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "X_standard = standard_scaler.fit_transform(X_preprocess)\n",
    "X_minmax = minmax_scaler.fit_transform(X_preprocess)\n",
    "\n",
    "# Visualize scaling effects\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "datasets = [X_preprocess, X_standard, X_minmax]\n",
    "titles = ['Original Data', 'StandardScaler', 'MinMaxScaler']\n",
    "\n",
    "for i, (data, title) in enumerate(zip(datasets, titles)):\n",
    "    bp = axes[i].boxplot(data, labels=['Feature 1', 'Feature 2', 'Feature 3'])\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].set_ylabel('Values')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Data Statistics Before and After Scaling:\")\n",
    "print(\"\\nOriginal Data:\")\n",
    "print(f\"Mean: {np.mean(X_preprocess, axis=0)}\")\n",
    "print(f\"Std:  {np.std(X_preprocess, axis=0)}\")\n",
    "\n",
    "print(\"\\nStandardScaler:\")\n",
    "print(f\"Mean: {np.mean(X_standard, axis=0)}\")\n",
    "print(f\"Std:  {np.std(X_standard, axis=0)}\")\n",
    "\n",
    "print(\"\\nMinMaxScaler:\")\n",
    "print(f\"Min: {np.min(X_minmax, axis=0)}\")\n",
    "print(f\"Max: {np.max(X_minmax, axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8bb4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial features demonstration\n",
    "# Generate 1D nonlinear data\n",
    "np.random.seed(42)\n",
    "X_poly_demo = np.random.uniform(-2, 2, 100).reshape(-1, 1)\n",
    "y_poly_demo = X_poly_demo.ravel()**3 + 0.5 * X_poly_demo.ravel()**2 + np.random.normal(0, 0.5, 100)\n",
    "\n",
    "# Apply polynomial features with different degrees\n",
    "degrees = [1, 2, 3, 4]\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "X_plot = np.linspace(-2, 2, 100).reshape(-1, 1)\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_poly = poly.fit_transform(X_poly_demo)\n",
    "    X_plot_poly = poly.transform(X_plot)\n",
    "    \n",
    "    # Fit linear regression on polynomial features\n",
    "    theta = np.linalg.pinv(X_poly.T @ X_poly) @ X_poly.T @ y_poly_demo\n",
    "    y_plot = X_plot_poly @ theta\n",
    "    \n",
    "    # Calculate R¬≤ score\n",
    "    y_pred = X_poly @ theta\n",
    "    r2 = RegressionMetrics.r2_score(y_poly_demo, y_pred)\n",
    "    \n",
    "    axes[i].scatter(X_poly_demo, y_poly_demo, alpha=0.6, label='Data')\n",
    "    axes[i].plot(X_plot, y_plot, color='red', linewidth=2, \n",
    "                label=f'Polynomial degree {degree}')\n",
    "    axes[i].set_title(f'Degree {degree}\\nR¬≤ = {r2:.3f}')\n",
    "    axes[i].set_xlabel('X')\n",
    "    axes[i].set_ylabel('y')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Polynomial Feature Engineering Effects')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c67ba72",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Proper evaluation of machine learning models is crucial. Let's explore different metrics and evaluation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916ce0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data for evaluation demo\n",
    "X_eval, y_eval = make_classification(n_samples=400, n_features=2, n_redundant=0, \n",
    "                                   n_informative=2, n_clusters_per_class=1, \n",
    "                                   random_state=42)\n",
    "X_eval_train, X_eval_test, y_eval_train, y_eval_test = train_test_split(\n",
    "    X_eval, y_eval, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler_eval = StandardScaler()\n",
    "X_eval_train_scaled = scaler_eval.fit_transform(X_eval_train)\n",
    "X_eval_test_scaled = scaler_eval.transform(X_eval_test)\n",
    "\n",
    "# Train multiple models for comparison\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(learning_rate=0.1, max_iterations=1000),\n",
    "    'KNN (k=3)': KNearestNeighbors(k=3),\n",
    "    'KNN (k=7)': KNearestNeighbors(k=7),\n",
    "    'KNN (k=15)': KNearestNeighbors(k=15),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    if 'Logistic' in name:\n",
    "        model.fit(X_eval_train_scaled, y_eval_train)\n",
    "        y_pred = model.predict(X_eval_test_scaled)\n",
    "        train_acc = model.score(X_eval_train_scaled, y_eval_train)\n",
    "        test_acc = model.score(X_eval_test_scaled, y_eval_test)\n",
    "    else:\n",
    "        model.fit(X_eval_train, y_eval_train)\n",
    "        y_pred = model.predict(X_eval_test)\n",
    "        train_acc = model.score(X_eval_train, y_eval_train)\n",
    "        test_acc = model.score(X_eval_test, y_eval_test)\n",
    "    \n",
    "    predictions[name] = y_pred\n",
    "    results[name] = {'train_acc': train_acc, 'test_acc': test_acc}\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    precision, recall, fscore = ClassificationMetrics.precision_recall_fscore(\n",
    "        y_eval_test, y_pred, average='binary')\n",
    "    results[name].update({'precision': precision, 'recall': recall, 'fscore': fscore})\n",
    "\n",
    "# Display results\n",
    "print(\"Model Comparison Results:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Model':<20} {'Train Acc':<10} {'Test Acc':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name:<20} {metrics['train_acc']:<10.3f} {metrics['test_acc']:<10.3f} \"\n",
    "          f\"{metrics['precision']:<10.3f} {metrics['recall']:<10.3f} {metrics['fscore']:<10.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cb4634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrices for each model\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (name, y_pred) in enumerate(predictions.items()):\n",
    "    cm = ClassificationMetrics.confusion_matrix(y_eval_test, y_pred)\n",
    "    \n",
    "    im = axes[i].imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    axes[i].figure.colorbar(im, ax=axes[i])\n",
    "    axes[i].set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]),\n",
    "               xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'],\n",
    "               title=f'{name}\\nAccuracy: {results[name][\"test_acc\"]:.3f}',\n",
    "               ylabel='True label', xlabel='Predicted label')\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2.\n",
    "    for row in range(cm.shape[0]):\n",
    "        for col in range(cm.shape[1]):\n",
    "            axes[i].text(col, row, format(cm[row, col], 'd'), ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm[row, col] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248a1d25",
   "metadata": {},
   "source": [
    "## Algorithm Comparison\n",
    "\n",
    "Let's compare all our algorithms on the same dataset to understand their strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57695a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive comparison\n",
    "datasets = {\n",
    "    'Linear Separable': make_classification(n_samples=300, n_features=2, n_redundant=0, \n",
    "                                          n_informative=2, n_clusters_per_class=1, \n",
    "                                          random_state=42),\n",
    "    'Non-linear': make_classification(n_samples=300, n_features=2, n_redundant=0, \n",
    "                                    n_informative=2, n_clusters_per_class=2, \n",
    "                                    random_state=42)\n",
    "}\n",
    "\n",
    "algorithms = {\n",
    "    'Logistic Regression': LogisticRegression(learning_rate=0.1, max_iterations=1000),\n",
    "    'KNN (k=5)': KNearestNeighbors(k=5),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "for dataset_idx, (dataset_name, (X_comp, y_comp)) in enumerate(datasets.items()):\n",
    "    X_comp_train, X_comp_test, y_comp_train, y_comp_test = train_test_split(\n",
    "        X_comp, y_comp, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Scale for logistic regression\n",
    "    scaler_comp = StandardScaler()\n",
    "    X_comp_train_scaled = scaler_comp.fit_transform(X_comp_train)\n",
    "    X_comp_test_scaled = scaler_comp.transform(X_comp_test)\n",
    "    \n",
    "    for alg_idx, (alg_name, model) in enumerate(algorithms.items()):\n",
    "        ax = axes[dataset_idx, alg_idx]\n",
    "        \n",
    "        if 'Logistic' in alg_name:\n",
    "            model.fit(X_comp_train_scaled, y_comp_train)\n",
    "            test_acc = model.score(X_comp_test_scaled, y_comp_test)\n",
    "            \n",
    "            # Plot decision boundary\n",
    "            h = 0.1\n",
    "            x_min, x_max = X_comp_train_scaled[:, 0].min() - 1, X_comp_train_scaled[:, 0].max() + 1\n",
    "            y_min, y_max = X_comp_train_scaled[:, 1].min() - 1, X_comp_train_scaled[:, 1].max() + 1\n",
    "            xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "            \n",
    "            mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "            Z = model.predict_proba(mesh_points)[:, 1]\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            \n",
    "            ax.contourf(xx, yy, Z, levels=50, alpha=0.6, cmap=plt.cm.RdYlBu)\n",
    "            ax.scatter(X_comp_train_scaled[:, 0], X_comp_train_scaled[:, 1], \n",
    "                      c=y_comp_train, cmap=plt.cm.RdYlBu, edgecolors='black')\n",
    "        else:\n",
    "            model.fit(X_comp_train, y_comp_train)\n",
    "            test_acc = model.score(X_comp_test, y_comp_test)\n",
    "            \n",
    "            # Plot decision boundary\n",
    "            h = 0.1\n",
    "            x_min, x_max = X_comp_train[:, 0].min() - 1, X_comp_train[:, 0].max() + 1\n",
    "            y_min, y_max = X_comp_train[:, 1].min() - 1, X_comp_train[:, 1].max() + 1\n",
    "            xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "            \n",
    "            mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "            Z = model.predict(mesh_points)\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            \n",
    "            ax.contourf(xx, yy, Z, alpha=0.6, cmap=plt.cm.RdYlBu)\n",
    "            ax.scatter(X_comp_train[:, 0], X_comp_train[:, 1], \n",
    "                      c=y_comp_train, cmap=plt.cm.RdYlBu, edgecolors='black')\n",
    "        \n",
    "        ax.set_title(f'{alg_name} on {dataset_name}\\nAccuracy: {test_acc:.3f}')\n",
    "        ax.set_xlabel('Feature 1')\n",
    "        ax.set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d505633",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### üéØ What You've Learned\n",
    "\n",
    "This tutorial covered the core machine learning algorithms available in ML101:\n",
    "\n",
    "#### Linear Regression\n",
    "- **Best for**: Continuous target variables with linear relationships\n",
    "- **Methods**: Normal Equation (exact) and Gradient Descent (scalable)\n",
    "- **Use cases**: Predicting house prices, stock prices, temperature\n",
    "\n",
    "#### Logistic Regression\n",
    "- **Best for**: Binary and multiclass classification\n",
    "- **Provides**: Probability estimates and interpretable coefficients\n",
    "- **Assumes**: Linear decision boundary in feature space\n",
    "\n",
    "#### K-Nearest Neighbors (KNN)\n",
    "- **Best for**: Non-linear decision boundaries\n",
    "- **Advantages**: No assumptions about data distribution\n",
    "- **Considerations**: Sensitive to choice of k and distance metric\n",
    "- **Computational cost**: High for large datasets (lazy learning)\n",
    "\n",
    "#### Data Preprocessing\n",
    "- **Scaling**: Critical for distance-based algorithms and gradient descent\n",
    "- **Feature engineering**: Polynomial features can help linear models\n",
    "- **Evaluation**: Multiple metrics provide comprehensive insight\n",
    "\n",
    "### üìä Algorithm Comparison\n",
    "\n",
    "| Algorithm | Linear Data | Non-linear Data | Speed | Interpretability | Memory |\n",
    "|-----------|-------------|-----------------|-------|------------------|--------|\n",
    "| Linear Regression | ‚úÖ | ‚ùå | ‚úÖ | ‚úÖ | ‚úÖ |\n",
    "| Logistic Regression | ‚úÖ | ‚ùå | ‚úÖ | ‚úÖ | ‚úÖ |\n",
    "| KNN | ‚úÖ | ‚úÖ | ‚ùå | ‚ùå | ‚ùå |\n",
    "\n",
    "### üîß When to Use Each Algorithm\n",
    "\n",
    "**Linear Regression**: When you need to predict continuous values and the relationship is roughly linear.\n",
    "\n",
    "**Logistic Regression**: When you need classification with probability estimates and interpretable results.\n",
    "\n",
    "**KNN**: When you have complex, non-linear patterns and don't need to understand the model's decision process.\n",
    "\n",
    "### üöÄ Next Steps with ML101\n",
    "\n",
    "1. **Explore More Algorithms**: \n",
    "   - Decision Trees and Random Forest\n",
    "   - Naive Bayes classifiers\n",
    "   - Support Vector Machines\n",
    "   - K-Means clustering\n",
    "   - Principal Component Analysis\n",
    "\n",
    "2. **Advanced Features**:\n",
    "   - Cross-validation techniques\n",
    "   - Hyperparameter tuning\n",
    "   - Feature selection methods\n",
    "   - Model ensemble techniques\n",
    "\n",
    "3. **Real-world Applications**:\n",
    "   - Apply to your own datasets\n",
    "   - Compare with scikit-learn\n",
    "   - Build complete ML pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4927ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ Congratulations! You've completed the ML101 tutorial!\")\n",
    "print(\"\\nüìö You've learned about:\")\n",
    "print(\"   ‚Ä¢ Linear Regression (Normal Equation & Gradient Descent)\")\n",
    "print(\"   ‚Ä¢ Logistic Regression (Binary Classification)\")\n",
    "print(\"   ‚Ä¢ K-Nearest Neighbors (Non-linear Classification)\")\n",
    "print(\"   ‚Ä¢ Data Preprocessing (Scaling & Feature Engineering)\")\n",
    "print(\"   ‚Ä¢ Model Evaluation (Metrics & Visualization)\")\n",
    "print(\"\\nüöÄ Next steps:\")\n",
    "print(\"   ‚Ä¢ Explore more algorithms: DecisionTree, RandomForest, NaiveBayes, SVM\")\n",
    "print(\"   ‚Ä¢ Try the example scripts in examples/\")\n",
    "print(\"   ‚Ä¢ Check out the comprehensive docs/ directory\")\n",
    "print(\"   ‚Ä¢ Apply these algorithms to real-world datasets\")\n",
    "print(\"\\nüì¶ ML101 Package:\")\n",
    "print(\"   ‚Ä¢ GitHub: https://github.com/yourusername/ML101\")\n",
    "print(\"   ‚Ä¢ PyPI: pip install ml101-algorithms\")\n",
    "print(\"   ‚Ä¢ Documentation: ./docs/algorithms/README.md\")\n",
    "\n",
    "print(\"\\n‚ú® Happy Machine Learning! ‚ú®\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
